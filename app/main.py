"""
Gradio UI for PowerGrid AI Tutor - HuggingFace Spaces version with API key input.
"""

import sys
import os
from pathlib import Path
import gradio as gr

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.rag.pipeline import RAGPipeline

class PowerGridTutorUI:
    """
    Gradio interface for the PowerGrid AI Tutor with API key input.
    """
    
    def __init__(self,
                 use_reranking: bool = True,
                 use_hybrid: bool = True,
                 use_query_expansion: bool = True,
                 llm_provider: str = "gemini"):
        """
        Initialize UI (pipeline created lazily after API key provided).
        """
        self.use_reranking = use_reranking
        self.use_hybrid = use_hybrid
        self.use_query_expansion = use_query_expansion
        self.llm_provider = llm_provider
        self.pipeline = None
        self.available_sources = []
        
    def initialize_pipeline(self, api_key: str, provider: str = "gemini", selected_features: list = None):
        """Initialize the RAG pipeline with the provided API key, provider, and selected features."""
        if not api_key or not api_key.strip():
            return f"âš ï¸ WARNING: Please provide a valid {provider.upper()} API key"
            
        try:
            # Set environment variable based on provider
            if provider == "gemini":
                os.environ['GOOGLE_API_KEY'] = api_key.strip()
            elif provider == "openai":
                os.environ['OPENAI_API_KEY'] = api_key.strip()
            
            # Update the provider
            self.llm_provider = provider
            
            # Parse selected features (strip emojis from feature names)
            if selected_features is None:
                selected_features = []
            selected_features_str = " ".join(selected_features)
            use_query_expansion = "Query Expansion" in selected_features_str
            use_hybrid = "Hybrid Search" in selected_features_str
            use_reranking = "Reranking" in selected_features_str
            
            # Initialize pipeline
            self.pipeline = RAGPipeline(
                use_reranking=use_reranking,
                use_hybrid=use_hybrid,
                use_query_expansion=use_query_expansion,
                llm_provider=provider,
                api_key=api_key.strip()
            )
            
            # Load existing vector store
            self.pipeline.load_existing(persist_dir="data/vector_stores/faiss_full")
            
            # Get available sources
            self.available_sources = self._get_available_sources()
            
            features = []
            if use_query_expansion:
                features.append("Query Expansion")
            if use_hybrid:
                features.append("Hybrid Search")
            if use_reranking:
                features.append("Reranking")
            
            features_str = " + ".join(features) if features else "Basic RAG"
            return f"âœ… SUCCESS! PowerGrid AI Tutor initialized successfully! ({self.llm_provider.upper()}: {features_str})"
            
        except Exception as e:
            return f"âŒ ERROR: {str(e)}"
    
    def _get_available_sources(self):
        """Get unique source files from the vector index."""
        if not self.pipeline:
            return []
        try:
            nodes = self.pipeline.index.docstore.docs.values()
            sources = set()
            for node in nodes:
                if hasattr(node, 'metadata') and 'source' in node.metadata:
                    sources.add(node.metadata['source'])
            return sorted(list(sources))
        except Exception as e:
            print(f"Warning: Could not retrieve sources: {e}")
            return []
    
    def chat(self, message, history, topic_filter, source_type_filter):
        """Handle chat messages with optional metadata filtering."""
        if not self.pipeline:
            return history + [{"role": "assistant", "content": " Please initialize the system with your API key first."}], message, gr.update()
            
        if not message.strip():
            return history, message, gr.update()

        # Build filters dictionary
        filters = {}
        if topic_filter and topic_filter != "All Topics":
            filters['topic'] = topic_filter.lower()

        try:
            # Start timing
            import time
            start_time = time.time()
            
            # Get answer from RAG pipeline with filters
            result = self.pipeline.query(message, filters=filters if filters else None, return_sources=True)
            
            # Calculate processing time
            processing_time = time.time() - start_time
            
            # Extract answer and sources
            if isinstance(result, dict):
                answer = result.get('answer', str(result))
                sources = result.get('sources', [])
                
                # Format response with sources
                response_text = answer
                
                # Only show sources if answer is meaningful (not rejection messages)
                if sources and "outside my area of expertise" not in answer.lower() and "outside the scope" not in answer.lower():
                    response_text += "\n\n**Sources:**\n"
                    for i, src in enumerate(sources[:3], 1):
                        # Handle both dict and NodeWithScore objects
                        if isinstance(src, dict):
                            source_file = src.get('source', 'Unknown')
                            score = src.get('score', 0)
                        else:
                            # NodeWithScore object
                            source_file = src.metadata.get('source', 'Unknown') if hasattr(src, 'metadata') else 'Unknown'
                            score = src.score if hasattr(src, 'score') else 0
                        
                        # Format relevance score as percentage
                        relevance_pct = f"{score * 100:.1f}%" if score > 0 else "N/A"
                        response_text += f"{i}. {source_file} (relevance: {relevance_pct})\n"
                
                # Add processing time
                response_text += f"\n*Processing time: {processing_time:.2f}s*"
            else:
                response_text = str(result)
                response_text += f"\n\n*Processing time: {processing_time:.2f}s*"
            
            # Append to history
            history.append({"role": "user", "content": message})
            history.append({"role": "assistant", "content": response_text})
            
            return history, "", gr.update(interactive=False)
        except Exception as e:
            error_msg = str(e)
            
            # Provide user-friendly error messages
            if "response.text" in error_msg or "safety_ratings" in error_msg:
                user_friendly_msg = "I'm here to help you with questions related to electrical engineering, renewable energy, and power systems. This question appears to be outside my area of expertise or may have been blocked by safety filters. If you have any questions about solar energy, wind power, batteries, smart grids, or related electrical engineering topics, please feel free to ask!"
            elif "does not contain" in error_msg.lower() or "no information" in error_msg.lower():
                user_friendly_msg = "I appreciate your interest, but I couldn't find relevant information in my knowledge base to answer this specific question. My expertise covers electrical engineering, renewable energy systems, power grids, and smart grid technologies. If you have questions in these areas, I'd be happy to help!"
            elif "quota" in error_msg.lower() or "rate limit" in error_msg.lower():
                user_friendly_msg = "âš ï¸ API rate limit reached. Please wait a moment and try again."
            elif "api key" in error_msg.lower() or "authentication" in error_msg.lower():
                user_friendly_msg = "ğŸ”‘ API key error. Please check your API key and try reinitializing."
            else:
                user_friendly_msg = f"âš ï¸ An error occurred: {error_msg}"
            
            history.append({"role": "user", "content": message})
            history.append({"role": "assistant", "content": user_friendly_msg})
            return history, "", gr.update(interactive=False)
    
    def create_interface(self):
        """Create and return the Gradio interface with API key input."""

        with gr.Blocks() as interface:
            
            # Header
            gr.HTML("""
                <div class="header">
                    <h1>âš¡ PowerGrid AI Tutor</h1>
                    <p>ğŸ“ Your Electrical Engineering & Renewable Energy Assistant</p>
                    <p style="font-size: 14px;">ğŸ¤– LLM: GEMINI | ğŸ“š Knowledge: 50 papers, 2166 chunks | âœ¨ Features: Query Expansion + Hybrid Search + Reranking</p>
                </div>
            """)
            
            # RAG Feature Selection
            with gr.Accordion("ğŸ”§ Advanced RAG Features", open=True):
                gr.Markdown("âœ¨ Enable advanced retrieval features (each adds API calls):")
                feature_checkboxes = gr.CheckboxGroup(
                    choices=[
                        "ğŸ” Query Expansion",
                        "ğŸ”€ Hybrid Search", 
                        "â­ Reranking"
                    ],
                    value=["ğŸ” Query Expansion", "ğŸ”€ Hybrid Search", "â­ Reranking"],
                    label="Select Features"
                )
            
            # API Key Input Section
            with gr.Row():
                provider_choice = gr.Dropdown(
                    choices=["gemini", "openai"],
                    value="gemini",
                    label="ğŸ¤– LLM Provider",
                    scale=1
                )
                api_key_input = gr.Textbox(
                    label="ğŸ”‘ API Key",
                    placeholder="Enter your API key here...",
                    type="password",
                    scale=3
                )
                init_button = gr.Button("ğŸš€ Initialize", scale=1, variant="primary")
            
            gr.Markdown("""
            **ğŸ”‘ Get API Keys:** 
            - **Gemini:** [Google AI Studio](https://aistudio.google.com/app/apikey) (Free tier: 15 RPM)
            - **OpenAI:** [OpenAI Platform](https://platform.openai.com/api-keys) (Paid, no region restrictions)
            """)
            
            status_message = gr.Textbox(label="ğŸ“Š Status", interactive=False, elem_classes="status-box")
            
            # Initialize button handler
            init_button.click(
                fn=self.initialize_pipeline,
                inputs=[api_key_input, provider_choice, feature_checkboxes],
                outputs=[status_message]
            )
            
            gr.Markdown("---")
            
            # Metadata filters
            with gr.Row():
                topic_filter = gr.Dropdown(
                    choices=["All Topics", "â˜€ï¸ Solar", "ğŸ’¨ Wind", "ğŸ”‹ Battery", "âš¡ Grid", "ğŸ“– General"],
                    value="All Topics",
                    label="ğŸ·ï¸ Filter by Topic",
                    scale=1
                )
            
            # Document type filter with checkboxes
            with gr.Accordion("ğŸ“„ Filter by Source Document Type", open=False):
                source_type_filter = gr.CheckboxGroup(
                    choices=["ğŸ“‘ Research Papers", "ğŸ“‹ Standards", "ğŸ“Š Technical Reports", "ğŸ“š Books"],
                    value=["ğŸ“‘ Research Papers", "ğŸ“‹ Standards", "ğŸ“Š Technical Reports", "ğŸ“š Books"],
                    label="Select document types to search"
                )

            # Main chat interface
            chatbot = gr.Chatbot(
                height=500,
                label="ğŸ’¬ Chat with PowerGrid AI Tutor",
                show_label=True
            )

            # Input area
            with gr.Row():
                msg = gr.Textbox(
                    placeholder="ğŸ’­ Ask me about solar energy, wind power, batteries, smart grids, or power systems...",
                    show_label=False,
                    scale=4
                )
                submit = gr.Button("ğŸ“¤ Send", scale=1, variant="primary", interactive=False)
            
            # Example questions
            gr.Examples(
                examples=[
                    "What are the main challenges in integrating solar power into the electrical grid?",
                    "How does wind energy affect power grid stability?",
                    "What are the latest advances in battery energy storage systems?",
                    "Explain smart grid technology and its benefits",
                    "What is the role of inverters in solar photovoltaic systems?"
                ],
                inputs=msg,
                label="ğŸ’¡ Example Questions"
            )
            
            # Clear button
            clear = gr.Button("ğŸ—‘ï¸ Clear Chat")
            
            # Event handlers
            def update_button_state(text):
                has_text = bool(text and text.strip())
                return gr.update(interactive=has_text)

            msg.input(fn=update_button_state, inputs=[msg], outputs=[submit])
            msg.change(fn=update_button_state, inputs=[msg], outputs=[submit])

            submit.click(
                fn=self.chat,
                inputs=[msg, chatbot, topic_filter, source_type_filter],
                outputs=[chatbot, msg, submit]
            )

            msg.submit(
                fn=self.chat,
                inputs=[msg, chatbot, topic_filter, source_type_filter],
                outputs=[chatbot, msg, submit]
            )

            clear.click(fn=lambda: [], outputs=[chatbot])
            
            # About section in accordion
            with gr.Accordion("â„¹ï¸ About PowerGrid AI Tutor", open=False):
                gr.Markdown("""
                ### ğŸ“– About
                This AI tutor uses state-of-the-art Retrieval-Augmented Generation (RAG) to answer questions
                about electrical engineering and renewable energy based on 50 research papers.

                ### ğŸ› ï¸ Technology Stack
                - **Vector Store:** FAISS
                - **Embeddings:** HuggingFace BAAI/bge-small-en-v1.5
                - **Framework:** LlamaIndex
                - **Features:** Metadata Filtering, Query Expansion, Hybrid Search, Reranking
                
                ### âœ¨ Features Active
                - **ğŸ” Query Expansion:** LLM generates technical terms for better keyword matching (+10-20% accuracy)
                - **ğŸ”€ Hybrid Search:** BM25 (keywords) + Semantic (meaning) with RRF fusion (+5-15% accuracy)
                - **â­ LLM Reranking:** LLM scores retrieved chunks for relevance (+15-25% accuracy)
                
                **ğŸ“ˆ Estimated Total Accuracy Gain:** +30-50% over baseline
                
                ### ğŸ”‘ API Keys Required
                You need either a **Google Gemini** or **OpenAI** API key to use this app:
                - **Gemini:** Get one for free at [Google AI Studio](https://aistudio.google.com/app/apikey)
                - **OpenAI:** Get one at [OpenAI Platform](https://platform.openai.com/api-keys) (paid)
                """)
        
        return interface
