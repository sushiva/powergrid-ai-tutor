# PowerGrid AI Tutor - Quick Reference Guide

## ğŸš€ Quick Start Commands

### Local Development

```bash
# Basic mode (semantic search only)
python app/main.py

# Advanced mode (all features)
python app/main.py --full

# With API key UI (for certification)
python app/main_with_api_key.py

# Using Ollama (free, local)
python app/main.py --llm ollama --full
```

### Feature Flags

```bash
# Individual features
python app/main.py --expand          # Query expansion
python app/main.py --hybrid          # Hybrid search  
python app/main.py --rerank          # LLM reranking

# Combine features
python app/main.py --expand --hybrid --rerank

# Full optimization
python app/main.py --full
```

### Testing

```bash
# Run evaluations
python evaluation/run_evaluation.py
python evaluation/compare_reranking.py

# Test specific features
python scripts/test_hybrid_search.py
python scripts/test_query_expansion.py
python scripts/test_metadata_filtering.py
```

---

## ğŸ“¦ Project Structure

```
powergrid-ai-tutor/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                   # Original Gradio UI (uses .env)
â”‚   â””â”€â”€ main_with_api_key.py      # Certification version (API key in UI) â­
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ loaders.py            # PDF loading
â”‚   â”‚   â”œâ”€â”€ chunkers.py           # Text chunking (512 tokens)
â”‚   â”‚   â”œâ”€â”€ embedders.py          # Embeddings + LLM setup
â”‚   â”‚   â””â”€â”€ metadata.py           # Metadata extraction
â”‚   â”‚
â”‚   â”œâ”€â”€ vector_store/
â”‚   â”‚   â””â”€â”€ faiss_store.py        # FAISS operations
â”‚   â”‚
â”‚   â””â”€â”€ rag/
â”‚       â”œâ”€â”€ pipeline.py           # Main RAG orchestrator â­
â”‚       â”œâ”€â”€ retrieval.py          # Hybrid retrieval
â”‚       â”œâ”€â”€ reranker.py           # LLM reranking
â”‚       â”œâ”€â”€ query_expander.py     # Query expansion
â”‚       â”œâ”€â”€ generator.py          # Answer generation
â”‚       â””â”€â”€ query_router.py       # Query routing (not active)
â”‚
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ datasets/
â”‚   â”‚   â”œâ”€â”€ test_queries.json     # 20 test queries
â”‚   â”‚   â””â”€â”€ ground_truth.json     # Expected answers
â”‚   â”œâ”€â”€ evaluators/
â”‚   â”‚   â”œâ”€â”€ hit_rate.py           # Hit Rate metric
â”‚   â”‚   â””â”€â”€ mrr.py                # MRR metric
â”‚   â”œâ”€â”€ run_evaluation.py         # Main evaluation
â”‚   â””â”€â”€ compare_reranking.py      # Compare performance
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ data_collection/
â”‚   â”‚   â””â”€â”€ collect_arxiv_papers.py  # ArXiv scraper
â”‚   â””â”€â”€ data_processing/
â”‚       â””â”€â”€ build_full_index.py      # Build FAISS index
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/papers/               # 50 PDF papers
â”‚   â””â”€â”€ vector_stores/
â”‚       â””â”€â”€ faiss_full/           # Pre-built index â­ (REQUIRED for HF)
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ deployment.md             # HuggingFace deployment guide â­
â”‚   â”œâ”€â”€ architecture.md
â”‚   â””â”€â”€ troubleshooting.md
â”‚
â”œâ”€â”€ README.md                     # Main documentation â­
â”œâ”€â”€ README_HF.md                  # For HuggingFace Space
â”œâ”€â”€ CERTIFICATION_CHECKLIST.md    # Requirements verification â­
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ requirements_hf.txt           # For HuggingFace deployment
â””â”€â”€ app.py                        # Entry point for HF Space
```

â­ = Critical files for certification

---

## ğŸ¯ Key Features & How to Use Them

### 1. Query Expansion
**What**: LLM generates technical synonyms before search  
**Enable**: `--expand` flag  
**Benefit**: +10-20% accuracy  
**Example**: "solar" â†’ "solar, photovoltaic, PV, solar cell, solar panel"

### 2. Hybrid Search
**What**: BM25 (keywords) + Semantic (meaning) with RRF fusion  
**Enable**: `--hybrid` flag  
**Benefit**: +5-15% accuracy  
**Why**: Catches both exact terms and semantic matches

### 3. LLM Reranking
**What**: LLM re-scores retrieved chunks for relevance  
**Enable**: `--rerank` flag  
**Benefit**: +15-25% accuracy  
**Trade-off**: Slower (adds 1-2s), but more accurate

### 4. Metadata Filtering
**What**: Filter by topic or source paper  
**Enable**: Always available in UI dropdowns  
**Topics**: Solar, Wind, Battery, Grid, General  
**Sources**: Any of the 50 research papers

### 5. Streaming Responses
**What**: Real-time answer generation  
**Enable**: Automatic in `main_with_api_key.py`  
**Benefit**: Better UX, see answers as they generate

---

## ğŸ§ª Evaluation Metrics

### Hit Rate @ K
- **Definition**: % of queries with at least 1 relevant chunk in top-K
- **Baseline**: 50% @ top-5
- **Full Pipeline**: ~70% @ top-5
- **Improvement**: +20 percentage points

### Mean Reciprocal Rank (MRR)
- **Definition**: Average of 1/rank of first relevant chunk
- **Baseline**: 33.9%
- **Full Pipeline**: ~55%
- **Improvement**: +21 percentage points

### Overall Accuracy Gain
- **Query Expansion**: +10-20%
- **Hybrid Search**: +5-15%
- **Reranking**: +15-25%
- **Combined**: +30-50%

---

## ğŸ’° Cost Analysis

### With Gemini API

| Component | Tokens/Query | Cost/Query |
|-----------|-------------|------------|
| Embeddings | 0 (local) | $0.000 |
| Query Expansion | ~500 | $0.0005 |
| Retrieval | 0 (local) | $0.000 |
| Reranking | ~800 | $0.0008 |
| Generation | ~1,200 | $0.0012 |
| **TOTAL** | **~2,500** | **~$0.003** |

**To try all features**: ~30 queries Ã— $0.003 = **< $0.10** âœ…

### With Ollama (Local)
- **Cost**: $0.00 (completely free)
- **Trade-off**: Slower (~30-40s per query vs 2-3s)

---

## ğŸ“‹ Certification Requirements

### Mandatory (All âœ…)
1. âœ… RAG project in Python
2. âœ… Uses LLM (Gemini/Ollama)
3. âš ï¸ HuggingFace deployment (ready, needs upload)
4. âœ… Data collection scripts
5. âœ… README with explanation
6. âœ… API key in UI (not in code)
7. âœ… No costly pipelines
8. âœ… Cost â‰¤ $0.50
9. âœ… API keys listed
10. âœ… â‰¥5 optional features

### Optional (8/5 Required âœ…)
1. âœ… Streaming responses
2. âœ… RAG evaluation
3. âœ… Domain-specific
4. âœ… Multiple data sources
5. âœ… Structured JSON
6. âœ… Reranker
7. âœ… Hybrid search
8. âœ… Metadata filtering

---

## ğŸš¢ Deployment Checklist

### Pre-Deployment
- [x] API key UI implemented
- [x] Streaming responses working
- [x] README complete
- [x] Cost estimation documented
- [x] Requirements files ready
- [x] Vector store built and saved

### HuggingFace Deployment Steps
1. Create new Space on HuggingFace
2. Clone space repository locally
3. Copy project files (see `docs/deployment.md`)
4. **CRITICAL**: Include `data/vector_stores/faiss_full/`
5. Set up Git LFS for large files
6. Push to HuggingFace
7. Test deployed app
8. Submit for certification

### Post-Deployment Testing
- [ ] Space loads without errors
- [ ] Can enter API key
- [ ] Can initialize system
- [ ] Chat responds correctly
- [ ] Filters work
- [ ] All features functional
- [ ] Cost is under $0.50 for demo

---

## ğŸ› Common Issues & Solutions

### "FAISS index not found"
**Solution**: Ensure `data/vector_stores/faiss_full/` is included in deployment

### "API key required"
**Solution**: This is expected! Users must provide their own key via UI

### "Git push failed - file too large"
**Solution**: Use Git LFS for vector store files
```bash
git lfs track "data/vector_stores/**/*"
```

### Slow first load
**Solution**: Normal - downloading embedding model (~150MB first time)

### Ollama connection error
**Solution**: Start Ollama service
```bash
ollama serve
ollama pull qwen2.5:7b
```

---

## ğŸ“Š Performance Benchmarks

### Response Time
- **Gemini (no features)**: 2-3 seconds
- **Gemini (full pipeline)**: 4-6 seconds
- **Ollama (no features)**: 30-40 seconds
- **Ollama (full pipeline)**: 60-90 seconds

### Accuracy
- **Baseline**: 50% Hit Rate
- **+ Query Expansion**: 60% Hit Rate
- **+ Hybrid Search**: 65% Hit Rate
- **+ Reranking**: 70% Hit Rate

### Knowledge Base
- **Papers**: 50
- **Pages**: 852
- **Chunks**: 2,166
- **Avg Chunk Size**: 512 tokens
- **Embedding Dim**: 384

---

## ğŸ“ Learning Resources

### RAG Concepts
- **Retrieval**: Finding relevant documents
- **Augmentation**: Adding context to prompt
- **Generation**: LLM creates answer from context

### Advanced Techniques
- **Hybrid Search**: Keyword + semantic retrieval
- **Reranking**: Re-score results for relevance
- **Query Expansion**: Add related terms to improve recall
- **Metadata Filtering**: Narrow search by attributes

### Evaluation Metrics
- **Hit Rate**: Coverage of relevant results
- **MRR**: Position of first relevant result
- **Precision**: % of retrieved docs that are relevant
- **Recall**: % of relevant docs that are retrieved

---

## ğŸ“ Support & Resources

**Documentation**: See `/docs` folder
**Checklist**: `CERTIFICATION_CHECKLIST.md`
**Deployment**: `docs/deployment.md`
**Troubleshooting**: `docs/troubleshooting.md`

**Repository**: https://github.com/sudhirshivaram/powergrid-ai-tutor

---

## ğŸ‰ You're Ready!

Your PowerGrid AI Tutor project is **95% complete** and ready for certification!

**Next Steps**:
1. Test locally: `python app/main_with_api_key.py`
2. Deploy to HuggingFace Spaces (see `docs/deployment.md`)
3. Test deployed version
4. Submit for certification

**Good luck!** ğŸš€
